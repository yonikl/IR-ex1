{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:29.938913Z",
     "start_time": "2024-12-06T12:31:28.092354Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('posts_first_targil.xlsx', sheet_name=None)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:29.966512Z",
     "start_time": "2024-12-06T12:31:29.958535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sheet_name, sheet_df in df.items():\n",
    "    print(f\"Sheet name: {sheet_name}\")\n",
    "    print(sheet_df.columns)"
   ],
   "id": "f7fce9efa774f4b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name: A-J\n",
      "Index(['sub_title', 'date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n",
      "Sheet name: BBC\n",
      "Index(['date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n",
      "Sheet name: J-P\n",
      "Index(['date', 'Newspaper', 'Body', 'title'], dtype='object')\n",
      "Sheet name: NY-T\n",
      "Index(['date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:30.062871Z",
     "start_time": "2024-12-06T12:31:30.055197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we're renaming J-P column to match others\n",
    "df['J-P'].rename(columns={'Body': 'Body Text'}, inplace=True)"
   ],
   "id": "77d65ade27251637",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:30.089226Z",
     "start_time": "2024-12-06T12:31:30.085125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sheet_name, sheet_df in df.items():\n",
    "    print(f\"Sheet name: {sheet_name}\")\n",
    "    print(sheet_df.columns)"
   ],
   "id": "4b3564b19c151695",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name: A-J\n",
      "Index(['sub_title', 'date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n",
      "Sheet name: BBC\n",
      "Index(['date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n",
      "Sheet name: J-P\n",
      "Index(['date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n",
      "Sheet name: NY-T\n",
      "Index(['date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:30.121318Z",
     "start_time": "2024-12-06T12:31:30.117410Z"
    }
   },
   "cell_type": "code",
   "source": "import re",
   "id": "2c5374a2ebdd5ac6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:30.141423Z",
     "start_time": "2024-12-06T12:31:30.136183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    border_pattern = r\"((?<!\\w)[^\\s\\w]|[^\\s\\w](?!\\w))\"\n",
    "    # Regex pattern\n",
    "    dot_pattern = r\"(?<!\\w)([a-zA-Z]{2,})\\.([a-zA-Z]{2,})(?!\\w)\"\n",
    "\n",
    "    # Apply regex substitution\n",
    "    cleaned_text = re.sub(dot_pattern, r\"\\1 . \\2\", text)\n",
    "\n",
    "    cleaned_text = re.sub(border_pattern, r\" \\1 \", cleaned_text)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    return re.sub(r\"\\s+\", \" \", cleaned_text).strip()"
   ],
   "id": "96f0408ddf710f38",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:31.496132Z",
     "start_time": "2024-12-06T12:31:30.158003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sheet_name, sheet_df in df.items():\n",
    "    sheet_df = sheet_df.map(lambda x: clean_text(x) if isinstance(x, str) else x)\n",
    "    sheet_df.to_csv(f'clean_data\\\\{sheet_name}.csv', index=False)"
   ],
   "id": "79c896c01665760f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. Lemmatization",
   "id": "f9d716a6e0bc484d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:33.936059Z",
     "start_time": "2024-12-06T12:31:31.516490Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk import word_tokenize, WordNetLemmatizer",
   "id": "e75b618a40becffa",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:33.985668Z",
     "start_time": "2024-12-06T12:31:33.979298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)  # Tokenize text\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in words])"
   ],
   "id": "f2fad3bee5a5a19e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:48.071605Z",
     "start_time": "2024-12-06T12:31:34.011821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "for file in os.listdir('./clean_data'):\n",
    "    print(file)\n",
    "    sheet = pd.read_csv(f'clean_data\\\\{file}')\n",
    "    sheet = sheet.map(lambda x: lemmatize_text(x) if isinstance(x, str) else x)\n",
    "    sheet.to_csv(f\"lemmatize_data\\\\{file}\", index=False)\n",
    "\n"
   ],
   "id": "d5021dcf2ef0b3d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-J.csv\n",
      "BBC.csv\n",
      "J-P.csv\n",
      "NY-T.csv\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BM25",
   "id": "f83cbf8ccab1a595"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:48.110191Z",
     "start_time": "2024-12-06T12:31:48.086613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "from scipy.sparse import save_npz\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "min_threshold = 5"
   ],
   "id": "8bd6416c0026cd8b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:48.132126Z",
     "start_time": "2024-12-06T12:31:48.126603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_metadata(word_index, output_file):\n",
    "    # save for feature use\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    metadata_df = pd.DataFrame(list(word_index.items()), columns=[\"Word\", \"Index\"])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    metadata_df.to_csv(output_file, index=False)\n",
    "    print(f\"Metadata saved to {output_file}\")\n"
   ],
   "id": "5d14bef5d38ec171",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:48.154164Z",
     "start_time": "2024-12-06T12:31:48.148960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_word_freq(corpus):\n",
    "    word_frequency = defaultdict(int)\n",
    "\n",
    "    # Count occurrences\n",
    "    for document in corpus:\n",
    "        unique_words = set(document)  # Use a set to avoid counting duplicates in the same document\n",
    "        for word in unique_words:\n",
    "            word_frequency[word] += 1\n",
    "\n",
    "    return  word_frequency"
   ],
   "id": "3ca85905980e3d5d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:31:48.179692Z",
     "start_time": "2024-12-06T12:31:48.174856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Tokenize the text\n",
    "    return [word.lower() for word in words if word.lower() not in stop_words]"
   ],
   "id": "76a6b0731c0315f6",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "bm25 to the lemmatized docs",
   "id": "2e7837946dd60962"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:32:01.678243Z",
     "start_time": "2024-12-06T12:31:48.201319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file in os.listdir('lemmatize_data'):\n",
    "    print(file)\n",
    "    sheet = pd.read_csv(f'lemmatize_data\\\\{file}')\n",
    "\n",
    "    # Build the corpus\n",
    "    if file == 'A-J.csv':\n",
    "        corpus = [\n",
    "            remove_stopwords(f'{row[\"title\"]} {row[\"sub_title\"]} {row[\"Body Text\"]}')\n",
    "            for _, row in sheet.iterrows()\n",
    "        ]\n",
    "    else:\n",
    "        corpus = [\n",
    "            remove_stopwords(f'{row[\"title\"]} {row[\"Body Text\"]}')\n",
    "            for _, row in sheet.iterrows()\n",
    "        ]\n",
    "\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "    word_frequency = get_word_freq(corpus)\n",
    "    all_words = [word for word in bm25.idf.keys() if word_frequency[word] > min_threshold]\n",
    "    word_index = {word: idx for idx, word in enumerate(all_words)}\n",
    "    save_metadata(word_index, f'metadata\\\\lemma\\\\{file}')\n",
    "\n",
    "    rows, cols, data = [], [], []\n",
    "    for word in all_words:\n",
    "        scores = bm25.get_scores(word)\n",
    "        for doc_idx, score in enumerate(scores):\n",
    "            if score > 0:  # Only include non-zero scores to keep it sparse\n",
    "                rows.append(doc_idx)  # Document index\n",
    "                cols.append(word_index[word])  # Word index\n",
    "                data.append(score)  # BM25 score\n",
    "\n",
    "    sparse_bm25_matrix = csr_matrix((data, (rows, cols)), shape=(len(corpus), len(all_words)))\n",
    "\n",
    "    save_npz(f'bm25\\\\lemma\\\\{file.split(\".\")[0]}', sparse_bm25_matrix)"
   ],
   "id": "a8c7ade03967579f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-J.csv\n",
      "Metadata saved to metadata\\lemma\\A-J.csv\n",
      "BBC.csv\n",
      "Metadata saved to metadata\\lemma\\BBC.csv\n",
      "J-P.csv\n",
      "Metadata saved to metadata\\lemma\\J-P.csv\n",
      "NY-T.csv\n",
      "Metadata saved to metadata\\lemma\\NY-T.csv\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "bm25 to the clean docs",
   "id": "4b8e023f360f8cc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:32:18.339438Z",
     "start_time": "2024-12-06T12:32:01.711860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file in os.listdir('clean_data'):\n",
    "    print(file)\n",
    "    sheet = pd.read_csv(f'clean_data\\\\{file}')\n",
    "\n",
    "    # Build the corpus\n",
    "    if file == 'A-J.csv':\n",
    "        corpus = [\n",
    "            remove_stopwords(f'{row[\"title\"]} {row[\"sub_title\"]} {row[\"Body Text\"]}') for _, row in sheet.iterrows()\n",
    "        ]\n",
    "    else:\n",
    "        corpus = [\n",
    "            remove_stopwords(f'{row[\"title\"]} {row[\"Body Text\"]}') for _, row in sheet.iterrows()\n",
    "        ]\n",
    "\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "    word_frequency = get_word_freq(corpus)\n",
    "    all_words = [word for word in bm25.idf.keys() if word_frequency[word] >= min_threshold]\n",
    "    word_index = {word: idx for idx, word in enumerate(all_words)}\n",
    "    save_metadata(word_index, f'metadata\\\\clean\\\\{file}')\n",
    "\n",
    "    rows, cols, data = [], [], []\n",
    "    for word in all_words:\n",
    "        scores = bm25.get_scores(word)\n",
    "        for doc_idx, score in enumerate(scores):\n",
    "            if score > 0:  # Only include non-zero scores to keep it sparse\n",
    "                rows.append(doc_idx)  # Document index\n",
    "                cols.append(word_index[word])  # Word index\n",
    "                data.append(score)  # BM25 score\n",
    "\n",
    "\n",
    "    sparse_bm25_matrix = csr_matrix((data, (rows, cols)), shape=(len(corpus), len(all_words)))\n",
    "\n",
    "    save_npz(f'bm25\\\\clean\\\\{file.split(\".\")[0]}', sparse_bm25_matrix)"
   ],
   "id": "9aba26f2b206e5bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-J.csv\n",
      "Metadata saved to metadata\\clean\\A-J.csv\n",
      "BBC.csv\n",
      "Metadata saved to metadata\\clean\\BBC.csv\n",
      "J-P.csv\n",
      "Metadata saved to metadata\\clean\\J-P.csv\n",
      "NY-T.csv\n",
      "Metadata saved to metadata\\clean\\NY-T.csv\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3: IG",
   "id": "77670b981639b51c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:32:18.445532Z",
     "start_time": "2024-12-06T12:32:18.367765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.sparse import load_npz\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n"
   ],
   "id": "12b6c42a5f096490",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "IG for lemmatized",
   "id": "42031e0bb3a7470f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:32:18.566158Z",
     "start_time": "2024-12-06T12:32:18.472428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file in os.listdir('bm25\\\\lemma'):\n",
    "    # Load the sparse BM25 matrix\n",
    "    sparse_bm25_matrix = load_npz(f'bm25\\\\lemma\\\\{file}')\n",
    "\n",
    "    # Sum the BM25 scores for each word across all documents (proxy for importance)\n",
    "    word_scores = np.array(sparse_bm25_matrix.sum(axis=0)).flatten()  # Sum along columns\n",
    "\n",
    "    # Load word-to-index mapping\n",
    "    word_metadata = pd.read_csv(f\"metadata\\\\lemma\\\\{file.split('.')[0]}.csv\")\n",
    "\n",
    "    # Map scores to words\n",
    "    df_word_scores = pd.DataFrame({\n",
    "        \"Word\": word_metadata[\"Word\"],\n",
    "        \"Score\": word_scores\n",
    "    }).sort_values(by=\"Score\", ascending=False)\n",
    "\n",
    "    # Save to CSV\n",
    "    df_word_scores.to_csv(f\"IG\\\\lemma\\\\{file.split('.')[0]}.csv\", index=False)\n",
    "\n",
    "    # Display top words by score\n",
    "    print(df_word_scores.head())"
   ],
   "id": "ba33fa91057d435c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Word        Score\n",
      "14          .  1140.336991\n",
      "13          ’   886.367915\n",
      "3           ,   857.445048\n",
      "295   country   173.383853\n",
      "117  saturday   173.383853\n",
      "              Word        Score\n",
      "4319      built-up  2582.526583\n",
      "3900     us-funded  2568.446341\n",
      "2155  unsuccessful  2499.764703\n",
      "3467       unusual  2473.820499\n",
      "1927         co.uk  2419.119081\n",
      "         Word        Score\n",
      "4061    1,200  2125.162599\n",
      "1205    1,500  2115.573944\n",
      "3477  100,000  2097.791597\n",
      "3510    1,400  2094.009603\n",
      "3594   10,000  2074.861862\n",
      "         Word       Score\n",
      "398     1,200  904.589510\n",
      "1076   35,000  899.313100\n",
      "14          ,  838.990794\n",
      "58          ’  649.898051\n",
      "294   u.c.l.a  253.451491\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "IG for clean",
   "id": "58b6b45561299730"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:32:18.719759Z",
     "start_time": "2024-12-06T12:32:18.630468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file in os.listdir('bm25\\\\clean'):\n",
    "    # Load the sparse BM25 matrix\n",
    "    sparse_bm25_matrix = load_npz(f'bm25\\\\clean\\\\{file}')\n",
    "\n",
    "    # Sum the BM25 scores for each word across all documents (proxy for importance)\n",
    "    word_scores = np.array(sparse_bm25_matrix.sum(axis=0)).flatten()  # Sum along columns\n",
    "\n",
    "    # Load word-to-index mapping\n",
    "    word_metadata = pd.read_csv(f\"metadata\\\\clean\\\\{file.split('.')[0]}.csv\")\n",
    "\n",
    "    # Map scores to words\n",
    "    df_word_scores = pd.DataFrame({\n",
    "        \"Word\": word_metadata[\"Word\"],\n",
    "        \"Score\": word_scores\n",
    "    }).sort_values(by=\"Score\", ascending=False)\n",
    "\n",
    "    # Save to CSV\n",
    "    df_word_scores.to_csv(f\"IG\\\\clean\\\\{file.split('.')[0]}.csv\", index=False)\n",
    "\n",
    "    # Display top words by score\n",
    "    print(df_word_scores.head())"
   ],
   "id": "e65c0174503433b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Word        Score\n",
      "13     .  1147.595128\n",
      "3      ,   862.656498\n",
      "104    :   163.927919\n",
      "130    ?   158.371471\n",
      "40     “   152.143006\n",
      "         Word        Score\n",
      "3676      1.7  2259.624247\n",
      "3680    1,700  2255.698013\n",
      "1965  576,000  2227.291317\n",
      "5191      2.7  2169.278964\n",
      "5476   37,000  2150.609325\n",
      "                        Word        Score\n",
      "1823       ldquo;we&rsquo;re  4182.916428\n",
      "1219        ldquo;it&rsquo;s  4139.154511\n",
      "1996         ldquo;i&rsquo;m  4139.154511\n",
      "3752        ben-gvir&rsquo;s  3011.835327\n",
      "682   ldquo;resistance&rdquo  2893.955323\n",
      "                          Word       Score\n",
      "274   storytelling,downloadthe  971.557259\n",
      "234      storytelling,download  943.473888\n",
      "419                      1,200  907.370295\n",
      "1255                    35,000  902.047393\n",
      "884                     30,000  886.152828\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Word2Vec",
   "id": "80261194d1fc927e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:32:46.032770Z",
     "start_time": "2024-12-06T12:32:18.725771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model (requires downloading)\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ],
   "id": "f42fd751a3f6c31c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:32:46.125092Z",
     "start_time": "2024-12-06T12:32:46.119751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \"\", text)      # Remove digits\n",
    "    # Tokenize the text\n",
    "    return text"
   ],
   "id": "55e14599ebe80030",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:33:02.263136Z",
     "start_time": "2024-12-06T12:32:46.162689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file in os.listdir('lemmatize_data'):\n",
    "    print(file)\n",
    "    sheet = pd.read_csv(f'lemmatize_data\\\\{file}')\n",
    "\n",
    "    if file == 'A-J.csv':\n",
    "        corpus = [\n",
    "            remove_stopwords(preprocess_text(f'{row[\"title\"]} {row[\"sub_title\"]} {row[\"Body Text\"]}')) for _, row in sheet.iterrows()\n",
    "        ]\n",
    "    else:\n",
    "        corpus = [\n",
    "            remove_stopwords(preprocess_text(f'{row[\"title\"]} {row[\"Body Text\"]}')) for _, row in sheet.iterrows()\n",
    "        ]\n",
    "\n",
    "    docs_matrix = []\n",
    "    for doc_idx, row in enumerate(corpus):\n",
    "        matrix = []\n",
    "        for word in row:\n",
    "            if word in model:\n",
    "                matrix.append(model[word].tolist())\n",
    "            else:\n",
    "                matrix.append([0] * model.vector_size)  # Placeholder for missing words\n",
    "\n",
    "        docs_matrix.append(np.array(matrix).mean(axis=0))\n",
    "    pd.DataFrame(docs_matrix).to_csv(f'Word2Vec\\\\lemma\\\\{file}', index=False)\n",
    "\n",
    "\n"
   ],
   "id": "931d2248bbdebff0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-J.csv\n",
      "BBC.csv\n",
      "J-P.csv\n",
      "NY-T.csv\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:33:18.068481Z",
     "start_time": "2024-12-06T12:33:02.299855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file in os.listdir('clean_data'):\n",
    "    print(file)\n",
    "    sheet = pd.read_csv(f'clean_data\\\\{file}')\n",
    "\n",
    "    if file == 'A-J.csv':\n",
    "        corpus = [\n",
    "            remove_stopwords(preprocess_text(f'{row[\"title\"]} {row[\"sub_title\"]} {row[\"Body Text\"]}')) for _, row in sheet.iterrows()\n",
    "        ]\n",
    "    else:\n",
    "        corpus = [\n",
    "            remove_stopwords(preprocess_text(f'{row[\"title\"]} {row[\"Body Text\"]}')) for _, row in sheet.iterrows()\n",
    "        ]\n",
    "    docs_matrix = []\n",
    "    for doc_idx, row in enumerate(corpus):\n",
    "        matrix = []\n",
    "        for word in row:\n",
    "            if word in model:\n",
    "                matrix.append(model[word].tolist())\n",
    "            else:\n",
    "                matrix.append([0] * model.vector_size)  # Placeholder for missing words\n",
    "\n",
    "        docs_matrix.append(np.array(matrix).mean(axis=0))\n",
    "\n",
    "    pd.DataFrame(docs_matrix).to_csv(f'Word2Vec\\\\clean\\\\{file}', index=False)\n",
    "\n"
   ],
   "id": "5f70299137a2d07a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-J.csv\n",
      "BBC.csv\n",
      "J-P.csv\n",
      "NY-T.csv\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Doc2Vec on original",
   "id": "67aadc36cf121f0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:33:18.107438Z",
     "start_time": "2024-12-06T12:33:18.102852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n"
   ],
   "id": "47fa2d2f040dffc5",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:33:29.908398Z",
     "start_time": "2024-12-06T12:33:18.172544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "original = df.copy()\n",
    "corpus = []\n",
    "j = 0\n",
    "for sheet_name, sheet_df in original.items():\n",
    "    # Prepare the corpus as a list of TaggedDocument objects\n",
    "    if sheet_name == 'A-J':\n",
    "        corpus += [\n",
    "            TaggedDocument(words=f'{row[\"title\"]} {row[\"sub_title\"]} {row[\"Body Text\"]}'.split(), tags=[str(i+j*1000)])\n",
    "            for i, row in sheet_df.iterrows()\n",
    "        ]\n",
    "    else:\n",
    "        corpus += [\n",
    "            TaggedDocument(words=f'{row[\"title\"]} {row[\"Body Text\"]}'.split(), tags=[str(i+j*1000)])\n",
    "            for i, row in sheet_df.iterrows()\n",
    "        ]\n",
    "    j+=1\n",
    "\n",
    "model = Doc2Vec(documents=corpus, vector_size=300, window=5, min_count=5, workers=4)\n",
    "\n",
    "document_vectors = [model.dv[tag] for tag in model.dv.index_to_key]\n",
    "\n",
    "# Save document vectors to a CSV file\n",
    "document_vectors = pd.DataFrame(document_vectors)\n",
    "document_vectors.iloc[0:599].to_csv(\"Doc2Vec\\\\A-J.csv\", index=False)\n",
    "document_vectors.iloc[599:1148].to_csv(\"Doc2Vec\\\\BBC.csv\", index=False)\n",
    "document_vectors.iloc[1148:1747].to_csv(\"Doc2Vec\\\\J-P.csv\", index=False)\n",
    "document_vectors.iloc[1747:2346].to_csv(\"Doc2Vec\\\\NY-T.csv\", index=False)"
   ],
   "id": "64a255f51da6de90",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BERT",
   "id": "b8c9feee1c21fe13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:33:39.133953Z",
     "start_time": "2024-12-06T12:33:29.919415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ],
   "id": "c58fc6cfe2394d68",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\PycharmProjects\\IR_ex1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:45:54.733528Z",
     "start_time": "2024-12-06T12:35:08.572970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "for sheet_name, sheet_df in original.items():\n",
    "    # Prepare the corpus as a list of TaggedDocument objects\n",
    "    if sheet_name == 'A-J':\n",
    "        corpus = [\n",
    "            f'{row[\"title\"]} {row[\"sub_title\"]} {row[\"Body Text\"]}'.lower() for _, row in sheet_df.iterrows()\n",
    "        ]\n",
    "    else:\n",
    "        corpus = [\n",
    "            f'{row[\"title\"]} {row[\"Body Text\"]}'.lower() for _, row in sheet_df.iterrows()\n",
    "        ]\n",
    "\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(device)\n",
    "    # model = model.to(device)\n",
    "    # Tokenize and encode the corpus\n",
    "    inputs = tokenizer(corpus, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract token embeddings from the last hidden state\n",
    "    last_hidden_state = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "    # Aggregate token embeddings to get sentence/document-level embeddings (e.g., mean pooling)\n",
    "    corpus_embeddings = torch.mean(last_hidden_state, dim=1)  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "    # Convert to numpy array for easier manipulation\n",
    "    corpus_embeddings = corpus_embeddings.numpy()\n",
    "\n",
    "    print(corpus_embeddings.shape)  # (num_documents, hidden_size)\n",
    "\n",
    "    pd.DataFrame(corpus_embeddings).to_csv(f\"Bert\\\\{sheet_name}.csv\", index=False)"
   ],
   "id": "724a61b387d706d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599, 768)\n",
      "(549, 768)\n",
      "(599, 768)\n",
      "(599, 768)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T13:08:13.821694Z",
     "start_time": "2024-12-06T13:08:02.877505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "for sheet_name, sheet_df in original.items():\n",
    "    # Prepare the corpus as a list of TaggedDocument objects\n",
    "    if sheet_name == 'A-J':\n",
    "        corpus = [\n",
    "            f'{row[\"title\"]} {row[\"sub_title\"]} {row[\"Body Text\"]}' for _, row in sheet_df.iterrows()\n",
    "        ]\n",
    "    else:\n",
    "        corpus = [\n",
    "            f'{row[\"title\"]} {row[\"Body Text\"]}' for _, row in sheet_df.iterrows()\n",
    "        ]\n",
    "\n",
    "    # Generate embeddings for the corpus\n",
    "    corpus_embeddings = model.encode(corpus)\n",
    "\n",
    "    # Check the shape of the embeddings\n",
    "    print(corpus_embeddings.shape)  # (num_documents, embedding_size)\n",
    "\n",
    "    pd.DataFrame(corpus_embeddings).to_csv(f\"Bert-Sentence\\\\{sheet_name}.csv\", index=False)"
   ],
   "id": "4673f8bf238b02e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599, 384)\n",
      "(549, 384)\n",
      "(599, 384)\n",
      "(599, 384)\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
